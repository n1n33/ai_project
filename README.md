# **EduRAG: Local Educational RAG System**
EduRAG — это автономная система вопросно-ответного поиска (Retrieval-Augmented Generation), разработанная для обработки образовательных материалов по точным наукам.

Проект обеспечивает полный цикл работы с документами: парсинг файлов (PDF, Markdown, DOCX), векторизацию контента и генерацию ответов с использованием LLM Qwen 2.5. Архитектура оптимизирована для локального развертывания на рабочих станциях с GPU NVIDIA (например, RTX 3060), обеспечивая полную конфиденциальность данных и отсутствие зависимости от внешних API.

Ключевые возможности
Локальная обработка данных: Все вычисления, включая генерацию эмбеддингов и инференс LLM, выполняются локально. Данные не передаются в облачные сервисы.

Контекстно-зависимая генерация: Реализован механизм History Aware Retriever, позволяющий модели учитывать историю диалога и корректно обрабатывать уточняющие запросы.

Мультиформатность: Поддержка индексации документов форматов .pdf, .docx, .txt, .md.

Кросслингвистический поиск: Использование модели intfloat/multilingual-e5-base позволяет выполнять семантический поиск по англоязычным документам при русскоязычных запросах.

GPU-акселерация: Использование библиотеки FAISS и PyTorch с поддержкой CUDA для ускорения векторизации и поиска.

Системные требования
Аппаратное обеспечение
GPU: NVIDIA GeForce RTX 3060 (12GB VRAM) или аналогичная с поддержкой CUDA 12.x.

RAM: Минимум 16 ГБ (рекомендуется 32 ГБ).

Накопитель: SSD (рекомендуется для быстрой загрузки весов моделей).

Программное обеспечение
ОС: Windows 10/11 (или Linux).

Python: Версия 3.10 или новее.

Ollama: Актуальная версия для запуска LLM.

NVIDIA Drivers: Версия 550.x или новее.

Установка и настройка
1. Подготовка LLM (Ollama)
Необходимо загрузить и настроить модель с расширенным контекстным окном.

Загрузите базовую модель:

Bash

ollama pull qwen2.5:14b-instruct-q3_K_M
Создайте конфигурацию модели. Убедитесь, что файл Modelfile находится в корне проекта, затем выполните:

Bash

ollama create edu-qwen-14b -f Modelfile
2. Настройка окружения Python
Рекомендуется использование виртуального окружения для изоляции зависимостей.

PowerShell

# Создание виртуального окружения
python -m venv .venv

# Активация окружения (Windows PowerShell)
.\.venv\Scripts\activate
3. Установка зависимостей
Установка выполняется в два этапа для корректной поддержки GPU.

Установка PyTorch с поддержкой CUDA 12.4:

PowerShell

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
Установка основных библиотек проекта:

PowerShell

pip install langchain==0.3.12 langchain-community==0.3.12 langchain-ollama langchain-huggingface faiss-cpu sentence-transformers streamlit streamlit-extras pypdf python-docx python-dotenv loguru
Конфигурация
Параметры работы системы задаются в файле config.yaml:

YAML

# Путь к директории с исходными документами для индексации
data_path: "data/raw/math_dataset"

# Модель для создания векторных представлений текста
embedding_model: "intfloat/multilingual-e5-base"
embedding_device: "cuda"

# Параметры разбиения текста (Chunking)
chunk_size: 800      # Количество символов в одном фрагменте
chunk_overlap: 150   # Перекрытие фрагментов для сохранения контекста
search_k: 5          # Количество релевантных фрагментов, передаваемых в LLM
Запуск приложения
Подготовка данных: Разместите целевые файлы документов в директорию, указанную в параметре data_path. Для автоматической загрузки тестового датасета можно использовать скрипт python download_data.py.

Запуск интерфейса: В активированном виртуальном окружении выполните:

PowerShell

streamlit run app.py
Инициализация индекса: В веб-интерфейсе (http://localhost:8501) нажмите кнопку "Пересобрать базу знаний". При первом запуске будет произведена загрузка модели эмбеддингов, что может занять некоторое время.

## Структура проекта

```text
edu_rag_project/
├── .venv/                  # Виртуальное окружение Python
├── data/
│   ├── raw/                # Директория для исходных документов
│   └── processed/          # Директория для хранения векторного индекса FAISS
├── src/                    # Исходный код модулей
│   ├── config.py           # Модуль загрузки конфигурации
│   ├── document_loader.py  # Модуль загрузки и парсинга документов
│   ├── rag_chain.py        # Логика RAG-цепочки и управления историей
│   └── vector_store.py     # Модуль работы с векторным хранилищем
├── app.py                  # Точка входа веб-приложения (Streamlit)
├── config.yaml             # Конфигурационный файл
├── download_data.py        # Утилита для загрузки датасетов
├── Modelfile               # Спецификация модели Ollama
└── README.md               # Документация проекта
```

## Устранение неполадок
Проблема: CUDA not available / Switching to CPU

Причина: Установлена версия PyTorch только для CPU.

Решение: Удалите текущую версию torch и выполните установку с флагом --index-url, указывающим на репозиторий CUDA 12.4 (см. раздел Установка).

Проблема: ModuleNotFoundError: No module named 'langchain.chains'

Причина: Несовместимость версий или некорректная установка пакетов.

Решение: Убедитесь, что используется виртуальное окружение .venv и установлены версии библиотек, указанные в инструкции.

Проблема: CUDA out of memory

Причина: Нехватка видеопамяти (VRAM).

Решение: Уменьшите параметр chunk_size в config.yaml или параметр num_ctx в Modelfile.

Проблема: Total files loaded: 0

Причина: Неверно указан путь к данным.

Решение: Проверьте параметр data_path в config.yaml. Путь должен указывать на конечную директорию, содержащую файлы.